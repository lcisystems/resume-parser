ASHOK GADIPARTHI                                  : ashok.gadiparthi@gmail.com                                                       :+919493206902       OBJECTIVE To provide BigData with Spark based Solution/Design for applications in different domains. I have gained good experience in full stack, Devops and Cloud solutions. I have took extensive training on Data Science technologies. use my ability to work in any environment. I seek opportunities that should challenge my talents and enable me to make significant contributions. I desire to work in stimulating team environments that promote the interchange of ideas.  PROFESSIONAL EXPERIENCE  Currently working with Boeing India, from Nov 2018 to till date in the designation of Senior Data Engineer.  Worked for JP morgan chase, from Mar 2016 to Nov 2018 in the designation of Associate.  Worked for Verizon Data Services India Pvt Ltd, from July 2012 to Mar 2016 in the designation of Analyst System Development.  Worked for Sparity Soft Solutions, Hyderabad from Oct 2011 to June 2012 in the designation – Software Engineer.  Worked for Fiables IT solutions, from Sep 2010 to Sep 2011 in the designation – Developer. PROFILE OVERVIEW  Over 10+ years of diversified IT experience in data analytics platforms such as as Bigdata, Hadoop, Python and Java development, In Aerospace, Banking, E-Commerce and Telecom domains.  Currently working on Bigdata infrastructure build out for batch processing as well as real-time processing.  Experience in designing and developing applications in Spark by using Java and Python to process structured or unstructured data at scale..  Experience in Goole Cloud Platform(GCP), CI/CD solutions like gitlab, docker, openshift, YML and PCF.  Good understanding of NoSQL databases/document stores and hands on experience in developing solutions on NoSQL components like MongoDB and Hbase.  Have knowledge on container based technologies like Docker and Openshift Kubernetes platforms.  Have experience on processing raw, structured, semi-structured and loosly coupled semi structured data at scale, including writing scripts and developing frameworks.  Implemented Microservices on RedHat OpenShift based on Kubernetes and PCF to achieve Continuous Delivery.  Hands on experience in Java,Python, Angular7 & SQL and Restful web services.  Have worked across multiple cross-functional teams in high visibility roles and own the data solution end-to-end.  Strong experience on Hadoop distributions like Hortonworks and Cloudera  Worked on New concepts to always be ahead with the technology on every step.  Quick learner and can adopt to any technologies.  Team player with good listening skills and innovative ideas to make things better. TECHNICAl SKILLS Education  Master of Computer Applications, University College of Engineering, Osmania University 2010 with 74% Hyderabad, India.  Bachelor of Sciences (Mathematics), Nagarjuna University 2007 with 72%, Guntur, India.  12th from, Chaitanya Junior & Degree College 2004 with 73%  10th from AVRPM high school 2002 with 78% Achievements & Certifications   Sun Certified Java Professional  PROJECTS PROFILE: BFLAx Flight Analytics Tools                  Hadoop, Java, PySpark ,Python, Hbase, MySQL, Oozie & Shell scripting   Cloud  GCP, Big Query, GCS, Data Flow and Airflow Role  Senior Data Engineer  Project Description: BFLAx (Flight Analytics) is an enterprise product with a mission to bring a common data management platform for all the data that can be directly or indirectly related to the flight sciences. The goal of this product is to enable and motivate our engineers & SMEs from various disciplines of flight sciences to adopt the latest technology and services     Project Role and Responsibilities:  Lead and develop data ingestion and curation frameworks solutions with big data technologies such as Spark and NoSQL technologies to meet process the unstructured Flight test data.  Involved in Hadoop to GCP migration progress from requirements gathering, estimation, design, development and deployment. Big Data/Full stack Tools Hadoop, Spark, Python, GCP, AWS, Angular7, MapReduce, HDFS, Storm, Kafka, Hive, Pig, Hbase, Sqoop, Flume, OOZIE, Ambari, Cassandra,  Phoenix,  Linux Scripting, Data Modelling, Splunk, J2SE, J2EE,  Devops, Jenkins, Gitlab, Openshift Hadoop Distributions Cloudera: 5.5, 5.7 and HortonWorks: 2.4 Hardware/ Operating System Windows XP, Windows 98/2000, UNIX, Linux Programming Languages and Tools JSP, Servlets ,JSF, JDBC, SQL, Struts, Springs, JSF, Webservices, XML, Junit, UML, HTML, Jquery, Java script, Maven, ANT, Eclipse Databases & Tools and application Servers  Oracle 10g, 11g ,  Mysql, DB2 , Tomcat, Weblogic, Jboss  Implemented python scripts to copy the data from Hadoop to Google Cloud Storage(GCS).  Implemented the curation logic to process semi structured flight science data using Python, GCS, Big Query Mysql and Data FLow.  Involved in complete Unit testing, Integration testing and peer code review.  Involved in requirements gathering from Flight sciences stake holders and design and development of on boarding flight test and wind tunnel data into Enterprise platform.  Build the complex relationships among different type of data files and implemented code to load data into Hadoop NoSQL database HBASE.   Contribute in design, code, configurations and documentation for components that manage data ingestion, batch processing, data extraction, transformation, and loading of different file formats.  Gather and process raw, structured, semi-structured, and unstructured data at scale, including writing scripts, developing programmatic interfaces against Rest APIs.  Work with the architect to define conceptual and logical data models.  ECFD:  Tools                  Hadoop, Hortaon works, OOZIE and Spark  Role  Senior Data Engineer  Project Description:     The Enterprise Computational Fluid Dynamics Infrastructure(eCFD) provides an interactive tool to support the creation of new processes and versions of process for use in the eCFD system. Project Role and Responsibilities: Worked as Senior Data Engineer  Involved in developing OOZIE work flow to copy data from HPC to Hadoop  Implemented Spark streaming to process real-time data for reporting.  Data Lineage:  Tools                   Angular7, Spring boot, D3, neo4J, Apache Airflow, pySpark and Python Services              Devops(CI/CD), gitlab, Openshift  Role  Senior Data Engineer  Project Description:     Data Lineage represents data journey which includes where the data originated and how it moved from one point to another and finally to its destination. Project Role and Responsibilities: Worked as Senior Data Engineer, full stack and Devops developer.  Involved in Devops implementation using gitlab and openshift (CI/CD).  Implemented entire UI portal using Angular7 and D3  Implemented back-end services using Spring boot and Neo4j.  Involved in Data process implementation using Airflow, pySpark and Python.  Provided the Hadoop based spark solutions for real-time data ingestion.  Fab2025 Tools                   Spring Boot, Angular7, Devops(CI/CD) with Gitlab, Docker and Mysql Services              Pivotal CLoud Foundry  Team Size           8 Role   Senior Data Engineer/Full stack   Project Description:     Deliver a data-driven, digital solution that enables analysts, decision makers and leaders in Fabrication to model future(e.g. rate and work statement changes) and visualize its impact on the business. Deliver accurate, real-time information using standard methods which details the impacts to fabrication machines, tools, facilities, raw materials and pople from demand changes for each MBU. Project Role and Responsibilities: Worked as Spring Boot, Data Engineer and Devops dveloper.  I have involved from application design, development and deployment.  I have handled the technical team of 5 members from assigning the requirments, code-review, unit and integration testing.  Involved in Devops implementation using gitlab, yml, Docker and PCF.  Worked on Spring boot to develop Back-end services.  Developed Data base design and development.  Implemented CI/CD pipeline for both UI and Back-end services on PCF  TRADE VAULT Client                  JP Morgan Chase - CIB Tools                   Hadoop, Spark, MapReduce, Scala, Casandra, HDFS, Kafka, Hive, Pig, Hbase, Sqoop, Flume, OOZIE, Spark, shell script. Services              Amazon cloud EC2  Team Size           23 Role   Associate  Project Description:     Trade Vault is a regulatory data warehouse responsible for global reporting in all regions, single consolidated order life cycle repository providing front to back linkage for data controls checks required by SEC and regulators in various Jurisdictions.   Problem statement for existing system cannot handle 1 billion messages per day, compute headroom of 20% increase and storage headroom of 9-10 months request to double capacity to 2 billion messages per day. Project Role and Responsibilities: Worked as Analyst System Development  Extensive experience in writing Spark Streaming and Spark Sql.  Involved in Devops implementation using Jenkins, Jira and Stash.  Worked on Order lining module by writing Kafka + Spark streaming + Hbase.  Developed kafka consumer utility for Data ingestion and OL.  Developed spark Sql utility to copy data from Hbase to HDFS and S3.  Developed Mapreduce import/export utility from Hbase staging to Hbase main tables bulk loading.  Involved in migration of serves from Exadata to Cloudera Impala.  Involved in Hive Optimization using Partitions,Bucketing,ORC and Parquet format.  Written pig script to load all the data from hdfs and loaded it in HIVE and Hbase tables.  Worked on the Handling high volume data processing.  Involved in offshore on requirements, design and development.   NPI (Net Work Performance Intelligence ) Client                  Verizon – Wire Line Tools                   Hadoop, MapReduce, HDFS, Storm, Kafka, Hive, Pig, Hbase, Sqoop, Flume, OOZIE, Spark, shell script. Team Size           15 Role   Analyst System Development Project Description:     The goal of network performance is to measure and improve the network performance, to ensure and improve the customer experience. To do that we creating network centric KPI's based on product performance features critical to customers. Because it is a huge network we need 99% of the observing done with software. we have to analyze the behavior of the network over time to predict events, find degradation, and impact of change to the network - both deliberate and accidental to find the problem before  to our customers do. Most important we need to take action based on what we learn to improve.  Project Role and Responsibilities: Worked as Analyst System Development  Extensive experience in writing with HiveQL and Pig scripts.  Involved in Hive Optimization using Partitions,Bucketing,ORC format.  Involved in Hbase phoneix optimization using Column Grouping, Salt bucketing, Compression and Indexing.  Involved in Pig script, OOZIE process Flow to perform actions like Java, Clean and pig actions.  Worked on Storm, Kafka and HBASE integration.  Developed Strom spouts and Bolts & involved in Kafka and Oracle golden gate integration.  Used various joins In HiveQL to get required output against the data provided.  Worked on transferring files from Different data sources to Hadoop file system using  Sqoop and Hive.  DWCDR (Data Warehouse for Call Detail Records) DWCDR (Aug 2012 to Dec 2014) Client                  Verizon – Wire Line Tools                   Hadoop, MapReduce, HDFS, Hive, Pig, Hbase, Sqoop, Flume, OOZIE, Ambari Team Size           20 Role   Analyst System Development Project Description:     DWCDR consists of an infrastructure to collect & store and tools to extract the business information from network call detail record data. The data can be accesses by Finance & Network internal user groups to identify trends, patterns, and opportunities for revenue enhancement and Telco cost reduction.  Project Role and Responsibilities: Worked as Analyst System Development  Written pig script to load all the data from hdfs and loaded it in HIVE and Hbase tables.  Worked on transferring files from Different data sources to Hadoop file system using  Sqoop and Hive.  Implemented the Correlation process with the help of Pig scripts, Hive, OOZIE and shell scripting.  WEB module implemented with the help of JSF, spring, and Hive.  Worked on the migration of sql queries from Netezza to Hive Queries.  Experience with setting up HIVE and HCATALOG, Pig, Ambari.  Verify (BPM-Business Process Management) Client                  Verizon – Wire Line Tools                   JSF, Rich faces, Web logic, Eclipse, Oracle 10g, Windows XP, Linux Team Size           30 Role   Analyst System Development  Project Description: The Verify project originated from a Verizon Business project to analyze Telco Cost Management business process and systems across the post-merger Verizon combined entity and develop a plan addressing a course of actions. Key functions in VERIFY: Invoice management, Account Management, Payment management, Audit & Exception Management, Dispute Management, Reference Data Management and User Administration. Project Role and Responsibilities: Worked as Analyst System Development  Worked on the Payment Management front end, Business Logic and Back end process    Developed GUI’s for Verify projects using JSF and Java script.  Developed the managed beans and process flow using the JSF.  Involved in fixing the production bugs as part of user testing.  Implemented the Export and download complements as part of the Reporting.  Retail Store Online Shopping Client                  Sportech, USA Tools                   Struts, Spring, Hibernate, MySql, Tomcat, Jquery, Linux, mysql. Team Size           10 Role   Developer  Project Description: The customer browses through the list of races, and places an order for them, through a third party, Equiibase.com, which is responsible for processing the request and delivering the product. On acceptance of the request, the product is delivered, in PDF or HTML format, through immediate download or mailed to the customer’s mail box.  The price of the product is then deducted from the customer’s account. Project Role and Responsibilities: Worked as Software Engineer   Worked on Front end and Business logic.  Worked on Shopping cart and transaction and order management.  Worked Struts Action and Spring auto wiring.  Fix the bugs as part of Testing.   